{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api import SeshatAPI\n",
    "import pandas as pd\n",
    "from ollama import chat, ChatResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for DeepSeek-R1\n",
    "\n",
    "Can a recent top performing LLM (DeepSeek-R1) correctly predict whether a variable from the Seshat Global History Databank (e.g. \"Scientific Literature\") should be \"present\" or \"absent\" for a selection of polities, given a definition of the variable, the name of the polity and the years in which it existed?\n",
    "\n",
    "If DeepSeek has a good understanding of history, can it use this to guess from the polity name and years what time/place the prompt is referring to, and whether the variable in question would have been present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = SeshatAPI(base_url=\"https://seshat-db.com/api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from Seshat\n",
    "\n",
    "First let's define our variable to be used in an LLM prompt later (taken from seshat-db.com), then load the data from the Seshat API to use as our ground truth to test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'Scientific Literature'\n",
    "definition = \"Talking about Kinds of Written Documents, Scientific literature includes mathematics, natural sciences, social sciences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api.sc import ScientificLiteratures\n",
    "scientific_literatures = ScientificLiteratures(client)\n",
    "scientific_literatures_df = pd.DataFrame(scientific_literatures.get_all())\n",
    "len(scientific_literatures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just use expert reviewed data and ignore examples where the value is anything other than \"present\" or \"absent\" to create a subsample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the records that are not expert reviewed\n",
    "scientific_literatures_df = scientific_literatures_df[scientific_literatures_df['expert_reviewed'] == True]\n",
    "print(len(scientific_literatures_df))\n",
    "\n",
    "# Filter out records where scientific_literature is not either 'present' or 'absent'\n",
    "scientific_literatures_df = scientific_literatures_df[scientific_literatures_df['scientific_literature'].isin(['present', 'absent'])]\n",
    "print(len(scientific_literatures_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just reformat the dataframe so we have information about the polities such as the start and end year alongside the variable value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the polities column to a new dataframe\n",
    "polities_with_scientific_literatures_df = pd.DataFrame(scientific_literatures_df['polity'].tolist())\n",
    "\n",
    "# Add the scientific_literature column to the new dataframe\n",
    "polities_with_scientific_literatures_df['scientific_literature'] = scientific_literatures_df['scientific_literature']\n",
    "print(len(polities_with_scientific_literatures_df))\n",
    "\n",
    "# Filter out records where scientific_literature is NaN\n",
    "polities_with_scientific_literatures_df = polities_with_scientific_literatures_df[polities_with_scientific_literatures_df['scientific_literature'].notna()]\n",
    "print(len(polities_with_scientific_literatures_df))\n",
    "\n",
    "# Get rid of the columns we don't need\n",
    "polities_with_scientific_literatures_df = polities_with_scientific_literatures_df[['new_name', 'long_name', 'start_year', 'end_year', 'scientific_literature', 'general_description']]\n",
    "\n",
    "polities_with_scientific_literatures_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Prompt engineering\"\n",
    "\n",
    "First let's define a prompt function to use with our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_CE(year):\n",
    "    if year >= 0:\n",
    "        return f\"{year} CE\"\n",
    "    else:\n",
    "        return f\"{abs(year)} BCE\"\n",
    "\n",
    "def prompt_func(seshat_df, polity_name, variable, variable_definition):\n",
    "    df = seshat_df[seshat_df['new_name'] == polity_name]\n",
    "    polity = list(df['long_name'])[0]\n",
    "    # description = list(df['general_description'])[0]  # TODO: we could add this to the prompt to add more context\n",
    "    start_year = list(df['start_year'])[0]\n",
    "    end_year = list(df['end_year'])[0]\n",
    "    prompt = \"Use your knowledge of world history to answer the following question. \"\n",
    "    prompt += f\"Given your knowledge of the historical polity '{polity}', \"\n",
    "    prompt += f\"a polity that existed between {year_CE(start_year)} and {year_CE(end_year)}\"\n",
    "    prompt += f\", do you expect that {variable} was present or absent? \"\n",
    "    prompt += f\"{variable} is defined as: '{variable_definition}'. \"\n",
    "    \n",
    "    # To help extract the answer from the text response later, make sure we have a string that can be found with a regex:\n",
    "    prompt += \"Answer 'XXXpresentXXX' if you expect it to be present, and 'XXXabsentXXX' if you expect it to be absent.\"\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the prompt look with an example \"new_name\" (Seshat ID) of `tn_fatimid_cal`, which we know has a record for scientific literature in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seshat_polity_name = 'tn_fatimid_cal'\n",
    "test_prompt = prompt_func(polities_with_scientific_literatures_df, test_seshat_polity_name, variable, definition)\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's see what DeepSeek responds for this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(model='deepseek-r1', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': test_prompt,\n",
    "  },\n",
    "])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was that correct? Let's check the dataframe to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_scientific_literatures_df[polities_with_scientific_literatures_df['new_name'] == test_seshat_polity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DeepSeek's answer data\n",
    "\n",
    "Let's write a function to collect responses from DeepSeek for all the polities in our dataset and make a new dataframe with the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_responses(seshat_df, variable, definition, count):  # Use the count parameter to limit the number of responses\n",
    "    def generate_response(polity):\n",
    "        response: ChatResponse = chat(model='deepseek-r1', messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt_func(seshat_df, polity, variable, definition),\n",
    "            },\n",
    "        ])\n",
    "        return response\n",
    "\n",
    "    responses = pd.DataFrame(columns=['new_name', 'answer', 'full'])\n",
    "    for polity in seshat_df['new_name']:\n",
    "        if len(responses) >= count:\n",
    "            break\n",
    "        response = generate_response(polity)\n",
    "        answer = \"\"\n",
    "        try:\n",
    "            answer = re.search(r'(XXXabsentXXX|XXXpresentXXX)', response.message.content).group(1)\n",
    "        except:\n",
    "            print(\"No answer found for\", polity)\n",
    "        answer = answer.replace(\"XXX\", \"\")\n",
    "        print(polity, \": \", answer)\n",
    "\n",
    "        # Add the new row to the DataFrame using pd.concat\n",
    "        responses = pd.concat([responses, pd.DataFrame([{\n",
    "            'new_name': polity,\n",
    "            'answer': answer,\n",
    "            'full': response.message.content\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the responses for the scientific literature variable\n",
    "scientific_literature_responses = deepseek_responses(polities_with_scientific_literatures_df, variable, definition, 2)\n",
    "scientific_literature_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for index, response in scientific_literature_responses.iterrows():\n",
    "    polity = response['new_name']\n",
    "    seshat_answer = polities_with_scientific_literatures_df[polities_with_scientific_literatures_df['new_name'] == polity]['scientific_literature'].values[0]\n",
    "    if seshat_answer == response['answer']:\n",
    "        correct += 1\n",
    "    print(polity, \": \", seshat_answer, response['answer'])\n",
    "    total += 1\n",
    "percentage = correct / total * 100\n",
    "print(f\"Correct: {correct}, Total: {total}, Percentage: {percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seshat_api_1)",
   "language": "python",
   "name": "seshat_api_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
